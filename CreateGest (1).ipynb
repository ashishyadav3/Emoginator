{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_x,image_y=50,50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(folder_name):\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.mkdir(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_images(g_id):\n",
    "    total_pics=1200\n",
    "    cap=cv2.VideoCapture(0)\n",
    "    x,y,w,h=300,50,350,350\n",
    "    create_folder(\"gestures/\"+str(g_id))\n",
    "    pic_no=0\n",
    "    flag_start_capturing=False\n",
    "    frames=0\n",
    "    while True:\n",
    "        ret,frame=cap.read()\n",
    "        frame=cv2.flip(frame,1)\n",
    "        hsv=cv2.cvtColor(frame,cv2.COLOR_BGR2HSV)\n",
    "        mask2=cv2.inRange(hsv,np.array([2,50,60]),np.array([25,150,255]))\n",
    "        res=cv2.bitwise_and(frame,frame,mask=mask2)\n",
    "        gray=cv2.cvtColor(res,cv2.COLOR_BGR2GRAY)\n",
    "        median=cv2.GaussianBlur(gray,(5,5),0)\n",
    "        kernel_square=np.ones((5,5),np.uint8)\n",
    "        dilation=cv2.dilate(median,kernel_square,iterations=2)\n",
    "        opening=cv2.morphologyEx(dilation,cv2.MORPH_CLOSE,kernel_square)\n",
    "        ret,thresh=cv2.threshold(opening,30,255,cv2.THRESH_BINARY)\n",
    "        thresh=thresh[y:y+h,x:x+w]\n",
    "        contours=cv2.findContours(thresh.copy(),cv2.RETR_TREE,cv2.CHAIN_APPROX_NONE)[1]\n",
    "        if len(contours)>0:\n",
    "            contour=max(contours,key=cv2.contourArea)\n",
    "            if cv2.contourArea(contour)>10000 and frames>50:\n",
    "                x1,y1,w1,h1=cv2.boundingRect(contour)\n",
    "                pic_no+=1\n",
    "                save_img=thresh[y1:y1+h1,x1:x1+w1]\n",
    "                save_img=cv2.resize(save_img,(image_x,image_y))\n",
    "                cv2.putText(frame,\"Capturing...\",(30,60),cv2.FONT_HERSHEY_TRIPLEX,2,(127,255,255))\n",
    "                cv2.imwrite(\"gestures/\"+str(g_id)+\"/\"+str(pic_no)+\".jpg\",save_img)\n",
    "            cv2.rectangle(frame,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "            cv2.putText(frame,str(pic_no),(30,400),cv2.FONT_HERSHEY_TRIPLEX,1.5,(127,127,255))\n",
    "            cv2.imshow(\"Capturing gesture\",frame)\n",
    "            cv2.imshow(\"thresh\",thresh)\n",
    "            keypress=cv2.waitKey(1)\n",
    "            if keypress==ord(\"c\"):\n",
    "                if flag_start_capturing==False:\n",
    "                    flag_start_capturing=True\n",
    "                else:\n",
    "                    flag_start_capturing=False\n",
    "                    frames=0\n",
    "            if flag_start_capturing==True:\n",
    "                frames+=1\n",
    "            if pic_no==total_pics:\n",
    "                break\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter gesture no:\n"
     ]
    }
   ],
   "source": [
    "g_id=input(\"Enter gesture no:\")\n",
    "store_images(g_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap=cv2.VideoCapture(0)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hand-emo/1\n",
      "hand-emo/2\n",
      "hand-emo/3\n",
      "hand-emo/4\n",
      "hand-emo/5\n",
      "hand-emo/6\n",
      "hand-emo/7\n",
      "hand-emo/8\n",
      "hand-emo/9\n",
      "hand-emo/10\n",
      "hand-emo/11\n",
      "hand-emo/12\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "j=0\n",
    "# default format can be changed as needed\n",
    "def createFileList(myDir, format='.jpg'):\n",
    "    fileList = []\n",
    "    print(myDir)\n",
    "    for root, dirs, files in os.walk(myDir, topdown=True):\n",
    "        for name in files:\n",
    "            if name.endswith(format):\n",
    "                fullName = os.path.join(root, name)\n",
    "                fileList.append(fullName)\n",
    "    return fileList\n",
    "\n",
    "# load the original image\n",
    "mydir=[\"hand-emo/1\",\"hand-emo/2\",\"hand-emo/3\",\"hand-emo/4\",\"hand-emo/5\",\"hand-emo/6\",\n",
    "       \"hand-emo/7\",\"hand-emo/8\",\"hand-emo/9\",\"hand-emo/10\",\"hand-emo/11\",\"hand-emo/12\"]\n",
    "for i in mydir:\n",
    "    j=j+1\n",
    "    myFileList=createFileList(i)\n",
    "    for file in myFileList:\n",
    "        img_file = Image.open(file)\n",
    "        #img_file.show()\n",
    "\n",
    "        # get original image parameters...\n",
    "        width, height = img_file.size\n",
    "        format = img_file.format\n",
    "        mode = img_file.mode\n",
    "\n",
    "        # Make image Greyscale\n",
    "        img_grey = img_file.convert('L')\n",
    "        #img_grey.save('result.png')\n",
    "        #img_grey.show()\n",
    "    \n",
    "        # Save Greyscale values\n",
    "        value = np.asarray(img_grey.getdata(), dtype=np.int).reshape((img_grey.size[1], img_grey.size[0]))\n",
    "        for m in range(1200):\n",
    "            value=np.append(value,j)\n",
    "            value = value.flatten()\n",
    "        with open(\"train_foo1.csv\", 'a') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Dense,Flatten,Conv2D\n",
    "from keras.layers import MaxPooling2D,Dropout\n",
    "from keras.utils import np_utils,print_summary\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import pandas as pd\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 2 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]]\n",
      "[ 6 10  8 ...  3  8  7]\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv(\"train_foo.csv\")\n",
    "dataset=np.array(data)\n",
    "np.random.shuffle(dataset)\n",
    "X=dataset\n",
    "Y=dataset\n",
    "X=X[:,0:2500]\n",
    "Y=Y[:,2500]\n",
    "print(X)\n",
    "print(Y)\n",
    "X_train=X[0:10800,:]\n",
    "X_train=X_train/255\n",
    "X_test=X[10800:12001,:]\n",
    "X_test=X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples=10800\n",
      "number of test examples:1201\n",
      "X_train shape:(10800, 2500)\n",
      "Y_train shape:(1, 10800)\n",
      "X_test shape:(1201, 2500)\n",
      "Y_test shape:(1, 1201)\n"
     ]
    }
   ],
   "source": [
    "Y=Y.reshape(Y.shape[0],1)\n",
    "Y_train=Y[0:10800,:]\n",
    "Y_train=Y_train.T\n",
    "Y_test=Y[10800:12001,:]\n",
    "Y_test=Y_test.T\n",
    "print(\"number of training examples=\"+str(X_train.shape[0]))\n",
    "print(\"number of test examples:\"+str(X_test.shape[0]))\n",
    "print(\"X_train shape:\"+str(X_train.shape))\n",
    "print(\"Y_train shape:\"+str(Y_train.shape))\n",
    "print(\"X_test shape:\"+str(X_test.shape))\n",
    "print(\"Y_test shape:\"+str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:(10800, 50, 50, 1)\n",
      "X_test shape:(1201, 50, 50, 1)\n",
      "train_y shape:(10800, 13)\n"
     ]
    }
   ],
   "source": [
    "image_x=50\n",
    "image_y=50\n",
    "train_y=np_utils.to_categorical(Y_train)\n",
    "test_y=np_utils.to_categorical(Y_test)\n",
    "train_y=train_y.reshape(train_y.shape[1],train_y.shape[2])\n",
    "test_y=test_y.reshape(test_y.shape[1],test_y.shape[2])\n",
    "X_train=X_train.reshape(X_train.shape[0],50,50,1)\n",
    "X_test=X_test.reshape(X_test.shape[0],50,50,1)\n",
    "print(\"X_train shape:\"+str(X_train.shape))\n",
    "print(\"X_test shape:\"+str(X_test.shape))\n",
    "print(\"train_y shape:\"+str(train_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def keras_model(image_x,image_y):\n",
    "    num_of_classes=13\n",
    "    model=Sequential()\n",
    "    model.add(Conv2D(32,(5,5),input_shape=(image_x,image_y,1),activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2),padding=\"same\"))\n",
    "    model.add(Conv2D(64,(5,5),activation=\"sigmoid\"))\n",
    "    model.add(MaxPooling2D(pool_size=(5,5),strides=(5,5),padding=\"same\"))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024,activation=\"relu\"))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(num_of_classes,activation=\"softmax\"))\n",
    "    model.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "    filepath=\"handEmo.h5\"\n",
    "    checkpoint1=ModelCheckpoint(filepath,monitor=\"val_acc\",verbose=1,save_best_only=True,mode=\"max\")\n",
    "    callbacks_list=[checkpoint1]\n",
    "    return model,callbacks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 10800 samples, validate on 1201 samples\n",
      "Epoch 1/1\n",
      "10800/10800 [==============================] - 68s 6ms/step - loss: 0.4920 - accuracy: 0.8679 - val_loss: 0.0032 - val_accuracy: 0.9992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ankit\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks\\callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Error:0.08%\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 46, 46, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 19, 19, 64)        51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 13)                13325     \n",
      "=================================================================\n",
      "Total params: 1,115,021\n",
      "Trainable params: 1,115,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model,callbacks_list=keras_model(image_x,image_y)\n",
    "model.fit(X_train,train_y,validation_data=(X_test,test_y),epochs=1,batch_size=64,callbacks=callbacks_list)\n",
    "scores=model.evaluate(X_test,test_y,verbose=0)\n",
    "print(\"CNN Error:%.2f%%\"%(100-scores[1]*100))\n",
    "print_summary(model)\n",
    "model.save(\"handEmo.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "model=load_model(\"handEmo.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emojis():\n",
    "    emoji_folder=\"hand-emo/\"\n",
    "    emojis=[]\n",
    "    for emoji in range(len(os.listdir(emoji_folder))):\n",
    "        print(emoji)\n",
    "        emojis.append(cv2.imread(emoji_folder+str(emoji)+\".jpg\",-1))\n",
    "    return emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "emojis=get_emojis()\n",
    "cap=cv2.VideoCapture(0)\n",
    "x,y,w,h=300,50,350,350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_predict(model,image):\n",
    "    processed=keras_process_image(image)\n",
    "    pred_probab=model.predict(processed)[0]\n",
    "    pred_class=list(pred_probab).index(max(pred_probab))\n",
    "    return max(pred_probab),pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def keras_process_image(img):\n",
    "    image_x=50\n",
    "    image_y=50\n",
    "    img=cv2.resize(img,(image_x,image_y))\n",
    "    img=np.array(img,dtype=np.float32)\n",
    "    img=np.reshape(img,(-1,image_x,image_y,1))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay(image,emoji,x,y,w,h):\n",
    "    emoji=cv2.resize(emoji,(w,h))\n",
    "    try:\n",
    "        image[y:y+h,x:x+w]=blend_transparent(image[y:y+h,x:x+w],emoji)\n",
    "    except:\n",
    "        pass\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def blend_transparent(face_img,overlay_t_img):\n",
    "    overlay_img=overlay_t_img[:,:,:3]\n",
    "    overlay_mask=overlay_t_img[:,:,3:]\n",
    "    background_mask=255-overlay_mask\n",
    "    overlay_mask=cv2.cvtColor(overlay_mask,cv2.COLOR_GRAY2BGR)\n",
    "    background_mask=cv2.cvtColor(background_mask,cv2.COLOR_GRAY2BGR)\n",
    "    face_part=(face_img*(1/255.0))*(backgroud_mask*(1/255.0))\n",
    "    overlay_part=(overlay_img*(1/255.0))*(overlay_mask*(1/255.0))\n",
    "    return np.uint8(cv.addWeighted(face_part,255.0,overlay_part,255.0,0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.5802255\n",
      "3 0.9938599\n",
      "3 0.7765638\n",
      "3 0.38286942\n",
      "4 0.25933787\n",
      "4 0.31183982\n",
      "4 0.28609073\n",
      "2 0.26468587\n",
      "2 0.28949267\n",
      "2 0.3267232\n",
      "2 0.31113592\n",
      "2 0.3221451\n",
      "11 0.31974307\n",
      "1 0.27916434\n",
      "1 0.4135609\n",
      "1 0.4419541\n",
      "1 0.34329528\n",
      "11 0.35695684\n",
      "1 0.2684657\n",
      "2 0.27929685\n",
      "11 0.36341327\n",
      "1 0.4054347\n",
      "1 0.31767702\n",
      "2 0.32252297\n",
      "1 0.3243647\n",
      "1 0.406521\n",
      "1 0.40442654\n",
      "1 0.51697916\n",
      "1 0.44860074\n",
      "1 0.49877396\n",
      "1 0.58772343\n",
      "1 0.53423536\n",
      "1 0.5680663\n",
      "1 0.564932\n",
      "1 0.50392\n",
      "1 0.48927253\n",
      "1 0.5770173\n",
      "1 0.5366761\n",
      "1 0.5082707\n",
      "1 0.53604937\n",
      "1 0.56380725\n",
      "1 0.5391534\n",
      "1 0.5173343\n",
      "1 0.39969242\n",
      "1 0.4438968\n",
      "1 0.34761053\n",
      "1 0.35959232\n",
      "2 0.2549862\n",
      "2 0.31733677\n",
      "1 0.29789606\n",
      "2 0.30007997\n",
      "1 0.3828362\n",
      "2 0.5553554\n",
      "2 0.38790175\n",
      "2 0.49452972\n",
      "2 0.40552557\n",
      "2 0.36384508\n",
      "1 0.5804091\n",
      "1 0.61291397\n",
      "1 0.76868755\n",
      "1 0.5961371\n",
      "1 0.6352153\n",
      "1 0.76481676\n",
      "1 0.77522343\n",
      "1 0.64090055\n",
      "1 0.8572273\n",
      "1 0.8713389\n",
      "1 0.92996335\n",
      "1 0.9156371\n",
      "1 0.9233053\n",
      "1 0.87009656\n",
      "1 0.8701376\n",
      "1 0.7089714\n",
      "11 0.5049707\n",
      "1 0.6011293\n",
      "1 0.4013033\n",
      "1 0.3907859\n",
      "11 0.3314662\n",
      "11 0.44513136\n",
      "4 0.29691797\n",
      "4 0.3108013\n",
      "11 0.28783292\n",
      "11 0.64023864\n",
      "11 0.53363574\n",
      "11 0.41588774\n",
      "6 0.59566087\n",
      "11 0.6445468\n",
      "11 0.49432045\n",
      "6 0.39152727\n",
      "6 0.27669314\n",
      "6 0.2620087\n",
      "6 0.3610108\n",
      "6 0.21542561\n",
      "4 0.28290048\n",
      "2 0.38430634\n",
      "1 0.559785\n",
      "4 0.3533724\n",
      "3 0.97934216\n",
      "3 0.99681634\n",
      "3 0.9986719\n",
      "3 0.99857163\n",
      "3 0.9890127\n",
      "3 0.9705495\n",
      "3 0.8982104\n",
      "3 0.91054946\n",
      "3 0.9057521\n",
      "3 0.682418\n",
      "3 0.87186646\n",
      "3 0.8622729\n",
      "3 0.9630779\n",
      "3 0.94965935\n",
      "3 0.9383373\n",
      "3 0.98551095\n",
      "3 0.9779092\n",
      "3 0.9584909\n",
      "3 0.9646605\n",
      "3 0.9835581\n",
      "3 0.9486341\n",
      "3 0.96756035\n",
      "3 0.9527874\n",
      "3 0.8783382\n",
      "3 0.8490935\n",
      "3 0.77509403\n",
      "3 0.7274089\n",
      "3 0.80661964\n",
      "3 0.87453\n",
      "3 0.6791364\n",
      "4 0.47769925\n",
      "9 0.38704646\n",
      "4 0.36796752\n",
      "9 0.38084498\n",
      "9 0.8945683\n",
      "9 0.9983072\n",
      "11 0.83381265\n",
      "9 0.9885688\n",
      "5 0.53457284\n",
      "4 0.36949167\n",
      "4 0.94172126\n",
      "4 0.9410763\n",
      "4 0.5002019\n",
      "4 0.57490164\n",
      "4 0.43895808\n",
      "4 0.5553362\n",
      "4 0.5064533\n",
      "4 0.47011095\n",
      "4 0.55444145\n",
      "8 0.3287798\n",
      "4 0.4979361\n",
      "4 0.37949285\n",
      "3 0.3502957\n",
      "4 0.48607966\n",
      "3 0.41341153\n",
      "3 0.9924891\n",
      "3 0.9991116\n",
      "3 0.9924811\n",
      "3 0.8611189\n",
      "9 0.8979068\n",
      "9 0.9736602\n",
      "9 0.9794638\n",
      "9 0.99423593\n",
      "9 0.98808354\n",
      "9 0.99406\n",
      "9 0.99710613\n",
      "9 0.99425596\n",
      "9 0.9953655\n",
      "9 0.99834144\n",
      "9 0.9968892\n",
      "9 0.9975184\n",
      "9 0.9971935\n",
      "9 0.99526644\n",
      "9 0.9980557\n",
      "9 0.9971892\n",
      "9 0.9770439\n",
      "3 0.6640695\n",
      "11 0.9912964\n",
      "11 0.91555214\n",
      "11 0.7088404\n",
      "6 0.9808445\n",
      "6 0.8552678\n",
      "2 0.7611284\n",
      "2 0.8786116\n",
      "2 0.94197834\n",
      "2 0.8558876\n",
      "5 0.53692126\n",
      "4 0.5027935\n",
      "4 0.51543564\n",
      "4 0.6594907\n",
      "4 0.7154011\n",
      "4 0.5938083\n",
      "4 0.515412\n",
      "2 0.7966767\n",
      "2 0.9574257\n",
      "2 0.9738463\n",
      "6 0.862514\n",
      "6 0.60768825\n",
      "6 0.9479402\n",
      "6 0.98001075\n",
      "6 0.659984\n",
      "7 0.52561074\n",
      "12 0.48233697\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-4098e7618945>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mpred_probab\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpred_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeras_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnewImage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_class\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpred_probab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0mimg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverlay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0memojis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpred_class\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m90\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m90\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m350\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m350\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Frame\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "while(cap.isOpened()):\n",
    "    ret,img=cap.read()\n",
    "    img=cv2.flip(img,1)\n",
    "    hsv=cv2.cvtColor(img,cv2.COLOR_BGR2HSV)\n",
    "    mask2=cv2.inRange(hsv,np.array([2,50,60]),np.array([25,150,255]))\n",
    "    res=cv2.bitwise_and(img,img,mask=mask2)\n",
    "    gray=cv2.cvtColor(res,cv2.COLOR_BGR2GRAY)\n",
    "    median=cv2.GaussianBlur(gray,(5,5),0)\n",
    "    kernel_square=np.ones((5,5),np.uint8)\n",
    "    dilation=cv2.dilate(median,kernel_square,iterations=2)\n",
    "    opening=cv2.morphologyEx(dilation,cv2.MORPH_CLOSE,kernel_square)\n",
    "    ret,thresh=cv2.threshold(opening,30,255,cv2.THRESH_BINARY)\n",
    "    thresh=thresh[y:y+h,x:x+h]\n",
    "    contours=cv2.findContours(thresh.copy(),cv2.RETR_TREE,cv2.CHAIN_APPROX_NONE)[1]\n",
    "    if len(contours)>0:\n",
    "        contour=max(contours,key=cv2.contourArea)\n",
    "        if cv2.contourArea(contour)>2500:\n",
    "            x,y,w1,h1=cv2.boundingRect(contour)\n",
    "            newImage=thresh[y:y+h1,x:x+w1]\n",
    "            newImage=cv2.resize(newImage,(50,50))\n",
    "            pred_probab,pred_class=keras_predict(model,newImage)\n",
    "            print(pred_class,pred_probab)\n",
    "            img=overlay(img,emojis[pred_class],400,250,90,90)\n",
    "    x,y,w,h=300,50,350,350\n",
    "    cv2.imshow(\"Frame\",img)\n",
    "    cv2.imshow(\"Contours\",thresh)\n",
    "    k=cv2.waitKey(10)\n",
    "    if k==27:\n",
    "        break\n",
    "keras_predict(model,np.zeros((50,50,1),dtype=np.uint8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap=cv2.VideoCapture(0)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
